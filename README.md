
# ğŸ“˜ edu_dbt_prj

**Education Analytics Data Platform (dbt + Elementary + Redshift-Ready POC)**

----------

## ğŸš€ Overview

`edu_dbt_prj` is a production-style **education analytics data platform POC** built using:

-   **dbt** (transformations, testing, snapshots)
    
-   **Postgres (local) â†’ Redshift-ready design**
    
-   **Elementary** (data observability + freshness monitoring)
    
-   **Docker**
    
-   **Automation via Makefile**
    
-   **CI-ready structure**
    

This project simulates a real-world **education business model** (similar to Colibri Group):

-   Students
    
-   Enrollments
    
-   Course activity
    
-   Course completion
    
-   Certifications
    
-   Payments
    

It produces business-ready marts:

-   ğŸ“Š Student Lifecycle (prospect â†’ enrolled â†’ active â†’ at-risk â†’ completed â†’ certified)
    
-   ğŸ“ˆ Certification KPIs (pass rate by attempt, avg days to certification)
    
-   ğŸ“œ Historical tracking via dbt Snapshots
    
-   ğŸ” Data freshness + quality observability via Elementary
    

----------

## ğŸ— Architecture
<img width="1237" height="600" alt="image" src="https://github.com/user-attachments/assets/904dadf2-d955-4ee1-b700-1ef54652977f" />

### ğŸ”¹ POC (Local)

Python Generator
        â†“
Raw Schemas (raw_lms, raw_billing, raw_cert)
        â†“
dbt Staging (stg_stg)
        â†“
Fact + Mart Layer (stg_marts)
        â†“
Snapshots (snapshots schema)
        â†“
Elementary Observability (stg schema)` 

----------

### ğŸ”¹ Production Equivalent

POC Component

Production Equivalent

Python Loader

Fivetran Connectors

Postgres

Amazon Redshift

Makefile

Airflow / dbt Cloud Jobs

Local Freshness

Scheduled Freshness Checks

Local HTML Report

Slack / Teams Alerts + Report Artifacts

----------

# ğŸ“‚ Repository Structure

edu_dbt_prj/
â”‚
â”œâ”€â”€ analytics/ # dbt project â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ snapshots/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ ingestion/ # Sample data generator + loader â”œâ”€â”€ warehouse/ddl/ # Raw schema DDL â”œâ”€â”€ scripts/ # Pipeline automation â”œâ”€â”€ docs/ # Demo walkthrough â”œâ”€â”€ edr_target/ # Elementary report output â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ requirements.txt

----------

# ğŸ§  Core Data Models

## ğŸ“Š fact_learning_activity_daily

Grain:

`student_id + course_id + activity_date` 

-   Incremental-ready
    
-   Merge strategy
    
-   Redshift dist/sort key compatible
    

----------

## ğŸ“ˆ mart_student_lifecycle_daily

Grain:

`student_id + as_of_date` 

Lifecycle stages:

-   prospect
    
-   enrolled
    
-   active
    
-   at_risk
    
-   completed
    
-   certified
    

Implements:

-   business rules
    
-   daily snapshot logic
    
-   incremental-ready design
    

----------

## ğŸ“ mart_certification_kpis

Provides:

-   Pass rate by attempt number
    
-   Avg days to certification
    
-   Certification-level metrics
    

----------

## ğŸ“œ snap_students (dbt Snapshot)

Implements:

-   Slowly Changing Dimension (Type 2)
    
-   Historical change tracking
    
-   `strategy='timestamp'`
    
-   `_ingested_at` as change detector
    

----------

# ğŸ” Data Observability (Elementary)

Configured features:

-   âœ… dbt tests monitoring
    
-   âœ… Source freshness SLAs
    
-   âœ… Model execution metadata
    
-   âœ… HTML report generation
    

Generate report:

`edr report --project-dir analytics --profiles-dir ~/.dbt` 

Output:

`edr_target/elementary_report.html` 

----------

# âš™ï¸ Local Setup

## 1ï¸âƒ£ Start Postgres

`make up` 

----------

## 2ï¸âƒ£ Setup Python

`python3 -m venv .venv source .venv/bin/activate
pip install -r requirements.txt` 

----------

## 3ï¸âƒ£ Configure dbt Profile

`mkdir -p ~/.dbt cp analytics/profiles.yml.example ~/.dbt/profiles.yml` 

Ensure port matches Docker config (default: 6543).

----------

## 4ï¸âƒ£ Run Full Pipeline

`make pipeline` 

This runs:

1.  Generate sample data
    
2.  Load raw schemas
    
3.  dbt build
    
4.  dbt source freshness
    
5.  Elementary report
    

----------

# ğŸ”„ CI/CD Ready

Includes:

-   Makefile automation
    
-   GitHub Actions compatible structure
    
-   dbt build + freshness checks
    
-   Redshift-ready incremental models
    

----------

# ğŸ§© Redshift Optimization Strategy

Prepared for production:

-   Incremental models with MERGE
    
-   Sort keys on date columns
    
-   Dist keys on student_id
    
-   Rolling window incremental loads
    
-   Schema sync on change
    
-   Freshness monitoring
    
-   Observability reporting
    

----------

# ğŸ›  Technologies Used

-   dbt (1.7)
    
-   Postgres (local warehouse)
    
-   Elementary (0.16)
    
-   Docker
    
-   Python 3.11
    
-   Makefile Automation
    

----------

# ğŸ“Œ Notes

-   This is a POC for demonstration/interview purposes.
    
-   In production:
    
    -   Replace Python ingestion with Fivetran
        
    -   Use Redshift instead of Postgres
        
    -   Run dbt via scheduler (Airflow/dbt Cloud)
        
    -   Enable Slack/Teams alerts in Elementary
        

----------

# ğŸ‘¤ Author

Manish Tiwari  
Senior Data Engineer | Analytics Engineering | Data Platform Architecture
