
# ğŸ“˜ edu_dbt_prj

**Education Analytics Data Platform (dbt + Elementary + Redshift-Ready POC)**

----------

## ğŸš€ Overview

`edu_dbt_prj` is a production-style **education analytics data platform POC** built using:

-   **dbt** (transformations, testing, snapshots)
    
-   **Postgres (local) â†’ Redshift-ready design**
    
-   **Elementary** (data observability + freshness monitoring)
    
-   **Docker**
    
-   **Automation via Makefile**
    
-   **CI-ready structure**
    

This project simulates a real-world **education business model** (similar to Colibri Group):

-   Students
    
-   Enrollments
    
-   Course activity
    
-   Course completion
    
-   Certifications
    
-   Payments
    

It produces business-ready marts:

-   ğŸ“Š Student Lifecycle (prospect â†’ enrolled â†’ active â†’ at-risk â†’ completed â†’ certified)
    
-   ğŸ“ˆ Certification KPIs (pass rate by attempt, avg days to certification)
    
-   ğŸ“œ Historical tracking via dbt Snapshots
    
-   ğŸ” Data freshness + quality observability via Elementary
    

----------

## ğŸ— Architecture

```mermaid
flowchart LR
  %% =========================
  %% POC (Local) Architecture
  %% =========================
  subgraph POC["POC Local"]
    A["Sample data generator\nPython"] --> B["Loader\nPython psycopg2"]
    B --> C[("Postgres warehouse\nDocker")]

    subgraph RAW["Raw layer"]
      C --> R1["raw_lms\nstudents, course_activity, course_completion"]
      C --> R2["raw_billing\nenrollments, payments"]
      C --> R3["raw_cert\nexam_attempts, certificates"]
    end

    subgraph DBT["dbt transformations"]
      R1 --> S1["stg_stg\nstaging views"]
      R2 --> S1
      R3 --> S1

      S1 --> F1["stg_marts.fact_learning_activity_daily\nincremental ready"]
      S1 --> M1["stg_marts.mart_student_lifecycle_daily"]
      S1 --> M2["stg_marts.mart_certification_kpis"]
    end

    subgraph SNAP["dbt snapshots"]
      R1 --> SS1["snapshots.snap_students\nSCD type 2"]
    end

    subgraph OBS["observability"]
      DBT --> FR["dbt source freshness\nSLA checks"]
      DBT --> TST["dbt tests\nschema and accepted values"]
      TST --> E1["Elementary tables\nschema stg"]
      FR --> E1
      E1 --> REP["Elementary HTML report\nedr_target/elementary_report.html"]
    end

    subgraph AUTO["automation"]
      MK["Makefile and scripts\nmake pipeline"] --> A
      MK --> DBT
      MK --> OBS
    end
  end

  %% =========================
  %% Production Mapping
  %% =========================
  subgraph PROD["Production mapping"]
    X["Fivetran connectors\nLMS billing certification"] --> Y[("Amazon Redshift")]
    Y --> Z1["Raw schemas\nraw or fivetran_raw"]
    Z1 --> Z2["Orchestration\nAirflow or dbt Cloud or Step Functions"]
    Z2 --> Z3["Staging marts snapshots\nfacts and dimensions"]
    Z2 --> Z4["Elementary alerting\nSlack or Teams"]
    Z3 --> BI["BI layer\nTableau Power BI Looker"]
  end

  %% Mapping
  C -. "Warehouse equivalent" .-> Y
  B -. "Ingestion replaced by" .-> X
  DBT -. "Same dbt patterns" .-> Z2
  OBS -. "Observability scaled" .-> Z4



### ğŸ”¹ POC (Local)

Python Generator
        â†“
Raw Schemas (raw_lms, raw_billing, raw_cert)
        â†“
dbt Staging (stg_stg)
        â†“
Fact + Mart Layer (stg_marts)
        â†“
Snapshots (snapshots schema)
        â†“
Elementary Observability (stg schema)` 

----------

### ğŸ”¹ Production Equivalent

POC Component

Production Equivalent

Python Loader

Fivetran Connectors

Postgres

Amazon Redshift

Makefile

Airflow / dbt Cloud Jobs

Local Freshness

Scheduled Freshness Checks

Local HTML Report

Slack / Teams Alerts + Report Artifacts

----------

# ğŸ“‚ Repository Structure

edu_dbt_prj/
â”‚
â”œâ”€â”€ analytics/ # dbt project â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ snapshots/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ ingestion/ # Sample data generator + loader â”œâ”€â”€ warehouse/ddl/ # Raw schema DDL â”œâ”€â”€ scripts/ # Pipeline automation â”œâ”€â”€ docs/ # Demo walkthrough â”œâ”€â”€ edr_target/ # Elementary report output â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ requirements.txt

----------

# ğŸ§  Core Data Models

## ğŸ“Š fact_learning_activity_daily

Grain:

`student_id + course_id + activity_date` 

-   Incremental-ready
    
-   Merge strategy
    
-   Redshift dist/sort key compatible
    

----------

## ğŸ“ˆ mart_student_lifecycle_daily

Grain:

`student_id + as_of_date` 

Lifecycle stages:

-   prospect
    
-   enrolled
    
-   active
    
-   at_risk
    
-   completed
    
-   certified
    

Implements:

-   business rules
    
-   daily snapshot logic
    
-   incremental-ready design
    

----------

## ğŸ“ mart_certification_kpis

Provides:

-   Pass rate by attempt number
    
-   Avg days to certification
    
-   Certification-level metrics
    

----------

## ğŸ“œ snap_students (dbt Snapshot)

Implements:

-   Slowly Changing Dimension (Type 2)
    
-   Historical change tracking
    
-   `strategy='timestamp'`
    
-   `_ingested_at` as change detector
    

----------

# ğŸ” Data Observability (Elementary)

Configured features:

-   âœ… dbt tests monitoring
    
-   âœ… Source freshness SLAs
    
-   âœ… Model execution metadata
    
-   âœ… HTML report generation
    

Generate report:

`edr report --project-dir analytics --profiles-dir ~/.dbt` 

Output:

`edr_target/elementary_report.html` 

----------

# âš™ï¸ Local Setup

## 1ï¸âƒ£ Start Postgres

`make up` 

----------

## 2ï¸âƒ£ Setup Python

`python3 -m venv .venv source .venv/bin/activate
pip install -r requirements.txt` 

----------

## 3ï¸âƒ£ Configure dbt Profile

`mkdir -p ~/.dbt cp analytics/profiles.yml.example ~/.dbt/profiles.yml` 

Ensure port matches Docker config (default: 6543).

----------

## 4ï¸âƒ£ Run Full Pipeline

`make pipeline` 

This runs:

1.  Generate sample data
    
2.  Load raw schemas
    
3.  dbt build
    
4.  dbt source freshness
    
5.  Elementary report
    

----------

# ğŸ”„ CI/CD Ready

Includes:

-   Makefile automation
    
-   GitHub Actions compatible structure
    
-   dbt build + freshness checks
    
-   Redshift-ready incremental models
    

----------

# ğŸ§© Redshift Optimization Strategy

Prepared for production:

-   Incremental models with MERGE
    
-   Sort keys on date columns
    
-   Dist keys on student_id
    
-   Rolling window incremental loads
    
-   Schema sync on change
    
-   Freshness monitoring
    
-   Observability reporting
    

----------

# ğŸ›  Technologies Used

-   dbt (1.7)
    
-   Postgres (local warehouse)
    
-   Elementary (0.16)
    
-   Docker
    
-   Python 3.11
    
-   Makefile Automation
    

----------

# ğŸ“Œ Notes

-   This is a POC for demonstration/interview purposes.
    
-   In production:
    
    -   Replace Python ingestion with Fivetran
        
    -   Use Redshift instead of Postgres
        
    -   Run dbt via scheduler (Airflow/dbt Cloud)
        
    -   Enable Slack/Teams alerts in Elementary
        

----------

# ğŸ‘¤ Author

Manish Tiwari  
Senior Data Engineer | Analytics Engineering | Data Platform Architecture
